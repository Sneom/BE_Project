{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOWyz3SQ6FVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xVfk-9YV6Fzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "autoencoder + classifier"
      ],
      "metadata": {
        "id": "K57EwtydMsJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import joblib\n",
        "\n",
        "# === Load and Preprocess Data ===\n",
        "df = pd.read_csv(\"new_network_train.csv\")\n",
        "X = df.drop(columns=[\"ProtocolName\"]).values\n",
        "y = df[\"ProtocolName\"].astype(np.int64).values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "\n",
        "# === Convert to PyTorch Tensors ===\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128)\n",
        "\n",
        "# === Autoencoder with Encoder Only for Classification ===\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "# === Model Init ===\n",
        "input_dim = X.shape[1]\n",
        "latent_dim = 64\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "encoder = Encoder(input_dim, latent_dim)\n",
        "decoder = Decoder(latent_dim, input_dim)\n",
        "classifier = Classifier(latent_dim, num_classes)\n",
        "\n",
        "# === Joint Optimizer and Losses ===\n",
        "params = list(encoder.parameters()) + list(decoder.parameters()) + list(classifier.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "recon_loss_fn = nn.MSELoss()\n",
        "clf_loss_fn = nn.CrossEntropyLoss()  # Can replace with weighted loss if needed\n",
        "\n",
        "# === Joint Training Loop ===\n",
        "print(\"\\nüîÅ Joint AE + Classifier Training\")\n",
        "for epoch in range(30):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoded = encoder(xb)\n",
        "        reconstructed = decoder(encoded)\n",
        "        logits = classifier(encoded)\n",
        "\n",
        "        loss_recon = recon_loss_fn(reconstructed, xb)\n",
        "        loss_clf = clf_loss_fn(logits, yb)\n",
        "\n",
        "        loss = loss_recon + loss_clf  # weight them if needed\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "encoder.eval()\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_test = encoder(X_test_tensor)\n",
        "    logits_test = classifier(encoded_test)\n",
        "    y_pred = torch.argmax(logits_test, dim=1).cpu().numpy()\n",
        "    y_true = y_test_tensor.cpu().numpy()\n",
        "\n",
        "label_map = {\n",
        "    0: 'AMAZON', 1: 'CLOUDFLARE', 2: 'DROPBOX', 3: 'FACEBOOK',\n",
        "    4: 'GMAIL', 5: 'GOOGLE', 6: 'HTTP', 7: 'HTTP_CONNECT', 8: 'HTTP_PROXY',\n",
        "    9: 'MICROSOFT', 10: 'MSN', 11: 'SKYPE', 12: 'SSL', 13: 'TWITTER',\n",
        "    14: 'WINDOWS_UPDATE', 15: 'YAHOO', 16: 'YOUTUBE'\n",
        "}\n",
        "print(\"\\n‚úÖ Final Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nüìä Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[label_map[i] for i in sorted(label_map)]))\n",
        "\n",
        "# === Save Models ===\n",
        "torch.save(encoder.state_dict(), \"encoder_joint.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder_joint.pth\")\n",
        "torch.save(classifier.state_dict(), \"classifier_joint.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWw5HgFE8TRQ",
        "outputId": "0c8dac4e-29e4-4664-e900-f6346afc148d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÅ Joint AE + Classifier Training\n",
            "Epoch 1, Total Loss: 1.8860\n",
            "Epoch 2, Total Loss: 1.5089\n",
            "Epoch 3, Total Loss: 1.3762\n",
            "Epoch 4, Total Loss: 1.3050\n",
            "Epoch 5, Total Loss: 1.2477\n",
            "Epoch 6, Total Loss: 1.2338\n",
            "Epoch 7, Total Loss: 1.1931\n",
            "Epoch 8, Total Loss: 1.1719\n",
            "Epoch 9, Total Loss: 1.1572\n",
            "Epoch 10, Total Loss: 1.1418\n",
            "Epoch 11, Total Loss: 1.1233\n",
            "Epoch 12, Total Loss: 1.1194\n",
            "Epoch 13, Total Loss: 1.1078\n",
            "Epoch 14, Total Loss: 1.0986\n",
            "Epoch 15, Total Loss: 1.0890\n",
            "Epoch 16, Total Loss: 1.0806\n",
            "Epoch 17, Total Loss: 1.0805\n",
            "Epoch 18, Total Loss: 1.0672\n",
            "Epoch 19, Total Loss: 1.0554\n",
            "Epoch 20, Total Loss: 1.0651\n",
            "Epoch 21, Total Loss: 1.0507\n",
            "Epoch 22, Total Loss: 1.0372\n",
            "Epoch 23, Total Loss: 1.0392\n",
            "Epoch 24, Total Loss: 1.0333\n",
            "Epoch 25, Total Loss: 1.0355\n",
            "Epoch 26, Total Loss: 1.0226\n",
            "Epoch 27, Total Loss: 1.0253\n",
            "Epoch 28, Total Loss: 1.0149\n",
            "Epoch 29, Total Loss: 1.0067\n",
            "Epoch 30, Total Loss: 1.0160\n",
            "\n",
            "‚úÖ Final Accuracy: 0.9350118835412953\n",
            "\n",
            "üìä Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        AMAZON       1.00      1.00      1.00      1584\n",
            "    CLOUDFLARE       1.00      0.92      0.96      1584\n",
            "       DROPBOX       0.97      0.87      0.92      1584\n",
            "      FACEBOOK       0.98      0.92      0.95      1584\n",
            "         GMAIL       0.89      0.99      0.93      1584\n",
            "        GOOGLE       0.95      0.94      0.94      1584\n",
            "          HTTP       1.00      1.00      1.00      1584\n",
            "  HTTP_CONNECT       0.95      0.91      0.93      1584\n",
            "    HTTP_PROXY       0.91      0.95      0.93      1584\n",
            "     MICROSOFT       0.93      1.00      0.96      1584\n",
            "           MSN       0.87      0.79      0.83      1584\n",
            "         SKYPE       0.82      0.94      0.88      1584\n",
            "           SSL       1.00      1.00      1.00      1584\n",
            "       TWITTER       0.92      0.95      0.93      1584\n",
            "WINDOWS_UPDATE       1.00      1.00      1.00      1584\n",
            "         YAHOO       0.81      0.89      0.84      1584\n",
            "       YOUTUBE       0.96      0.85      0.90      1584\n",
            "\n",
            "      accuracy                           0.94     26928\n",
            "     macro avg       0.94      0.94      0.94     26928\n",
            "  weighted avg       0.94      0.94      0.94     26928\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder + LSTM"
      ],
      "metadata": {
        "id": "Blj5j1jBaypy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import joblib\n",
        "\n",
        "# === Load and Preprocess Data ===\n",
        "df = pd.read_csv(\"new_network_train.csv\")\n",
        "X = df.drop(columns=[\"ProtocolName\"]).values\n",
        "y = df[\"ProtocolName\"].astype(np.int64).values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128)\n",
        "\n",
        "# === Autoencoder + LSTM Classifier ===\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(latent_dim, 128, batch_first=True, num_layers=2, dropout=0.3)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Convert to (batch_size, seq_len=1, latent_dim)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        return self.fc(h_n[-1])  # Use last hidden state\n",
        "\n",
        "# === Model Init ===\n",
        "input_dim = X.shape[1]\n",
        "latent_dim = 64\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "encoder = Encoder(input_dim, latent_dim)\n",
        "decoder = Decoder(latent_dim, input_dim)\n",
        "classifier = LSTMClassifier(latent_dim, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "classifier = classifier.to(device)\n",
        "\n",
        "# === Joint Optimizer and Loss ===\n",
        "params = list(encoder.parameters()) + list(decoder.parameters()) + list(classifier.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "recon_loss_fn = nn.MSELoss()\n",
        "clf_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# === Joint Training Loop ===\n",
        "print(\"\\nüîÅ Joint AE + LSTM Training\")\n",
        "for epoch in range(30):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoded = encoder(xb)                       # (batch, latent_dim)\n",
        "        reconstructed = decoder(encoded)           # (batch, input_dim)\n",
        "        logits = classifier(encoded)               # (batch, num_classes)\n",
        "\n",
        "        loss_recon = recon_loss_fn(reconstructed, xb)\n",
        "        loss_clf = clf_loss_fn(logits, yb)\n",
        "        loss = loss_recon + loss_clf\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Total Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "encoder.eval()\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_test = encoder(X_test_tensor.to(device))\n",
        "    logits_test = classifier(encoded_test)\n",
        "    y_pred = torch.argmax(logits_test, dim=1).cpu().numpy()\n",
        "    y_true = y_test_tensor.cpu().numpy()\n",
        "\n",
        "label_map = {\n",
        "    0: 'AMAZON', 1: 'CLOUDFLARE', 2: 'DROPBOX', 3: 'FACEBOOK',\n",
        "    4: 'GMAIL', 5: 'GOOGLE', 6: 'HTTP', 7: 'HTTP_CONNECT', 8: 'HTTP_PROXY',\n",
        "    9: 'MICROSOFT', 10: 'MSN', 11: 'SKYPE', 12: 'SSL', 13: 'TWITTER',\n",
        "    14: 'WINDOWS_UPDATE', 15: 'YAHOO', 16: 'YOUTUBE'\n",
        "}\n",
        "\n",
        "print(\"\\n‚úÖ Final Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nüìä Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[label_map[i] for i in sorted(label_map)]))\n",
        "\n",
        "# === Save Models ===\n",
        "torch.save(encoder.state_dict(), \"encoder_lstm_joint.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder_lstm_joint.pth\")\n",
        "torch.save(classifier.state_dict(), \"classifier_lstm_joint.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuR_Tj9CGLt",
        "outputId": "3b8a3e98-5cbe-4267-f9ca-d1be6f30234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÅ Joint AE + LSTM Training\n",
            "Epoch 1, Total Loss: 2.0205\n",
            "Epoch 2, Total Loss: 1.6445\n",
            "Epoch 3, Total Loss: 1.5618\n",
            "Epoch 4, Total Loss: 1.5220\n",
            "Epoch 5, Total Loss: 1.4792\n",
            "Epoch 6, Total Loss: 1.4563\n",
            "Epoch 7, Total Loss: 1.4266\n",
            "Epoch 8, Total Loss: 1.4158\n",
            "Epoch 9, Total Loss: 1.3934\n",
            "Epoch 10, Total Loss: 1.3803\n",
            "Epoch 11, Total Loss: 1.3643\n",
            "Epoch 12, Total Loss: 1.3437\n",
            "Epoch 13, Total Loss: 1.3390\n",
            "Epoch 14, Total Loss: 1.3021\n",
            "Epoch 15, Total Loss: 1.2959\n",
            "Epoch 16, Total Loss: 1.2551\n",
            "Epoch 17, Total Loss: 1.2409\n",
            "Epoch 18, Total Loss: 1.2275\n",
            "Epoch 19, Total Loss: 1.1927\n",
            "Epoch 20, Total Loss: 1.1882\n",
            "Epoch 21, Total Loss: 1.1697\n",
            "Epoch 22, Total Loss: 1.1566\n",
            "Epoch 23, Total Loss: 1.1442\n",
            "Epoch 24, Total Loss: 1.1312\n",
            "Epoch 25, Total Loss: 1.1289\n",
            "Epoch 26, Total Loss: 1.1233\n",
            "Epoch 27, Total Loss: 1.1082\n",
            "Epoch 28, Total Loss: 1.0966\n",
            "Epoch 29, Total Loss: 1.1018\n",
            "Epoch 30, Total Loss: 1.0916\n",
            "\n",
            "‚úÖ Final Accuracy: 0.8886660724896019\n",
            "\n",
            "üìä Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        AMAZON       1.00      1.00      1.00      1584\n",
            "    CLOUDFLARE       0.82      0.98      0.89      1584\n",
            "       DROPBOX       0.93      0.84      0.88      1584\n",
            "      FACEBOOK       0.87      0.95      0.91      1584\n",
            "         GMAIL       0.87      0.86      0.87      1584\n",
            "        GOOGLE       0.76      0.90      0.83      1584\n",
            "          HTTP       1.00      1.00      1.00      1584\n",
            "  HTTP_CONNECT       0.92      0.89      0.90      1584\n",
            "    HTTP_PROXY       0.89      0.92      0.91      1584\n",
            "     MICROSOFT       0.98      0.79      0.87      1584\n",
            "           MSN       0.89      0.76      0.82      1584\n",
            "         SKYPE       0.79      0.78      0.78      1584\n",
            "           SSL       1.00      1.00      1.00      1584\n",
            "       TWITTER       0.82      0.77      0.80      1584\n",
            "WINDOWS_UPDATE       1.00      1.00      1.00      1584\n",
            "         YAHOO       0.79      0.90      0.84      1584\n",
            "       YOUTUBE       0.85      0.75      0.80      1584\n",
            "\n",
            "      accuracy                           0.89     26928\n",
            "     macro avg       0.89      0.89      0.89     26928\n",
            "  weighted avg       0.89      0.89      0.89     26928\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gans + cnn"
      ],
      "metadata": {
        "id": "fA_fsAefbi5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv(\"new_network_train.csv\")\n",
        "X = df.drop(columns=[\"ProtocolName\"]).values\n",
        "y = df[\"ProtocolName\"].astype(np.int64).values\n",
        "\n",
        "# === Preprocessing ===\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "latent_dim = 100\n",
        "label_smooth_real = 0.9\n",
        "\n",
        "# === One-hot encoder ===\n",
        "def one_hot(labels, num_classes):\n",
        "    return torch.eye(num_classes, device=labels.device)[labels]\n",
        "\n",
        "# === Focal Loss ===\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logpt = F.log_softmax(input, dim=1)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = (1 - pt) ** self.gamma * logpt\n",
        "        return F.nll_loss(loss, target)\n",
        "\n",
        "# === Generator ===\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, label_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim + label_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        labels = one_hot(labels, num_classes).to(noise.device)\n",
        "        x = torch.cat([noise, labels], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# === Discriminator ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, label_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + label_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, data, labels):\n",
        "        labels = one_hot(labels, num_classes).to(data.device)\n",
        "        x = torch.cat([data, labels], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# === Classifier ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# === Init Models ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(latent_dim, num_classes, input_dim).to(device)\n",
        "D = Discriminator(input_dim, num_classes).to(device)\n",
        "clf = CNNClassifier(input_dim, num_classes).to(device)\n",
        "\n",
        "# === Losses and Optimizers ===\n",
        "bce = nn.BCELoss()\n",
        "focal_loss = FocalLoss()\n",
        "opt_G = optim.Adam(G.parameters(), lr=0.0002)\n",
        "opt_D = optim.Adam(D.parameters(), lr=0.0002)\n",
        "opt_C = optim.Adam(clf.parameters(), lr=0.001)\n",
        "\n",
        "# Schedulers\n",
        "scheduler_C = optim.lr_scheduler.StepLR(opt_C, step_size=15, gamma=0.5)\n",
        "\n",
        "# === Training Loop ===\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    G.train(); D.train(); clf.train()\n",
        "    total_d_loss, total_g_loss, total_c_loss = 0, 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        batch_size = xb.size(0)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        real_labels = torch.full((batch_size, 1), label_smooth_real).to(device)\n",
        "        fake_labels = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_data = G(z, yb)\n",
        "\n",
        "        D_real = D(xb, yb)\n",
        "        D_fake = D(fake_data.detach(), yb)\n",
        "\n",
        "        loss_D = bce(D_real, real_labels) + bce(D_fake, fake_labels)\n",
        "        opt_D.zero_grad()\n",
        "        loss_D.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        D_fake = D(fake_data, yb)\n",
        "        loss_G = bce(D_fake, real_labels)\n",
        "        opt_G.zero_grad()\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # === Train Classifier on real + fake ===\n",
        "        synthetic_data = G(torch.randn(batch_size, latent_dim).to(device), yb)\n",
        "        combined_data = torch.cat([xb, synthetic_data], dim=0)\n",
        "        combined_labels = torch.cat([yb, yb], dim=0)\n",
        "\n",
        "        preds = clf(combined_data)\n",
        "        loss_C = focal_loss(preds, combined_labels)\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "        total_d_loss += loss_D.item()\n",
        "        total_g_loss += loss_G.item()\n",
        "        total_c_loss += loss_C.item()\n",
        "\n",
        "    scheduler_C.step()\n",
        "    print(f\"[Epoch {epoch+1}] D Loss: {total_d_loss:.4f}, G Loss: {total_g_loss:.4f}, C Loss: {total_c_loss:.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "clf.eval()\n",
        "with torch.no_grad():\n",
        "    preds = clf(X_test_tensor.to(device))\n",
        "    y_pred = preds.argmax(dim=1).cpu().numpy()\n",
        "    y_true = y_test_tensor.cpu().numpy()\n",
        "\n",
        "label_map = {\n",
        "    0: 'AMAZON', 1: 'CLOUDFLARE', 2: 'DROPBOX', 3: 'FACEBOOK',\n",
        "    4: 'GMAIL', 5: 'GOOGLE', 6: 'HTTP', 7: 'HTTP_CONNECT', 8: 'HTTP_PROXY',\n",
        "    9: 'MICROSOFT', 10: 'MSN', 11: 'SKYPE', 12: 'SSL', 13: 'TWITTER',\n",
        "    14: 'WINDOWS_UPDATE', 15: 'YAHOO', 16: 'YOUTUBE'\n",
        "}\n",
        "print(\"\\n‚úÖ Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nüìä Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[label_map[i] for i in sorted(label_map)]))\n",
        "\n",
        "# === Save Models ===\n",
        "torch.save(G.state_dict(), \"generator_cgan2.pth\")\n",
        "torch.save(D.state_dict(), \"discriminator_cgan2.pth\")\n",
        "torch.save(clf.state_dict(), \"classifier_cgan2.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKKhhAUQUPrU",
        "outputId": "f9d85740-5c7f-4b6e-f74a-3b7da9a323c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] D Loss: 658.8518, G Loss: 2413.1014, C Loss: 1437.5552\n",
            "[Epoch 2] D Loss: 656.2894, G Loss: 2336.0910, C Loss: 1322.8202\n",
            "[Epoch 3] D Loss: 652.2607, G Loss: 2153.4269, C Loss: 1275.4349\n",
            "[Epoch 4] D Loss: 667.2299, G Loss: 2128.5513, C Loss: 944.4518\n",
            "[Epoch 5] D Loss: 656.3497, G Loss: 2019.6076, C Loss: 699.2260\n",
            "[Epoch 6] D Loss: 699.9287, G Loss: 1833.4889, C Loss: 591.2645\n",
            "[Epoch 7] D Loss: 744.1338, G Loss: 1701.0849, C Loss: 484.7723\n",
            "[Epoch 8] D Loss: 777.9539, G Loss: 1671.4868, C Loss: 459.4634\n",
            "[Epoch 9] D Loss: 790.6512, G Loss: 1552.3411, C Loss: 456.4172\n",
            "[Epoch 10] D Loss: 793.9263, G Loss: 1565.2438, C Loss: 438.2249\n",
            "[Epoch 11] D Loss: 795.9860, G Loss: 1593.8712, C Loss: 442.2619\n",
            "[Epoch 12] D Loss: 793.4390, G Loss: 1587.5938, C Loss: 400.5894\n",
            "[Epoch 13] D Loss: 802.1005, G Loss: 1610.5773, C Loss: 370.0211\n",
            "[Epoch 14] D Loss: 807.5804, G Loss: 1599.9068, C Loss: 347.7442\n",
            "[Epoch 15] D Loss: 824.2466, G Loss: 1555.1756, C Loss: 332.6735\n",
            "[Epoch 16] D Loss: 830.7850, G Loss: 1576.1654, C Loss: 325.6839\n",
            "[Epoch 17] D Loss: 838.3388, G Loss: 1545.4209, C Loss: 314.7828\n",
            "[Epoch 18] D Loss: 833.3452, G Loss: 1566.1561, C Loss: 313.1521\n",
            "[Epoch 19] D Loss: 837.5335, G Loss: 1565.6346, C Loss: 306.0688\n",
            "[Epoch 20] D Loss: 841.5798, G Loss: 1566.1349, C Loss: 296.0042\n",
            "[Epoch 21] D Loss: 837.3021, G Loss: 1587.7432, C Loss: 294.5785\n",
            "[Epoch 22] D Loss: 842.9844, G Loss: 1576.9506, C Loss: 283.4771\n",
            "[Epoch 23] D Loss: 845.2496, G Loss: 1573.4259, C Loss: 280.0284\n",
            "[Epoch 24] D Loss: 849.6136, G Loss: 1574.6897, C Loss: 268.1523\n",
            "[Epoch 25] D Loss: 845.4536, G Loss: 1585.0341, C Loss: 266.6255\n",
            "[Epoch 26] D Loss: 854.6713, G Loss: 1572.5577, C Loss: 261.0807\n",
            "[Epoch 27] D Loss: 850.2101, G Loss: 1586.9807, C Loss: 261.6931\n",
            "[Epoch 28] D Loss: 850.8803, G Loss: 1585.8724, C Loss: 249.9165\n",
            "[Epoch 29] D Loss: 863.8703, G Loss: 1568.3867, C Loss: 249.4640\n",
            "[Epoch 30] D Loss: 861.9170, G Loss: 1556.1913, C Loss: 239.4048\n",
            "[Epoch 31] D Loss: 855.2723, G Loss: 1577.5398, C Loss: 232.3310\n",
            "[Epoch 32] D Loss: 864.0404, G Loss: 1554.2544, C Loss: 230.0723\n",
            "[Epoch 33] D Loss: 865.7312, G Loss: 1546.7287, C Loss: 223.7645\n",
            "[Epoch 34] D Loss: 872.3385, G Loss: 1541.1643, C Loss: 223.8884\n",
            "[Epoch 35] D Loss: 871.4746, G Loss: 1534.7415, C Loss: 226.3121\n",
            "[Epoch 36] D Loss: 875.4979, G Loss: 1535.0174, C Loss: 218.1327\n",
            "[Epoch 37] D Loss: 882.4737, G Loss: 1506.1167, C Loss: 213.1816\n",
            "[Epoch 38] D Loss: 885.1838, G Loss: 1493.3071, C Loss: 213.2815\n",
            "[Epoch 39] D Loss: 893.8220, G Loss: 1478.0826, C Loss: 210.4409\n",
            "[Epoch 40] D Loss: 895.6022, G Loss: 1465.4605, C Loss: 207.4263\n",
            "[Epoch 41] D Loss: 904.3801, G Loss: 1444.7694, C Loss: 207.6182\n",
            "[Epoch 42] D Loss: 910.0212, G Loss: 1434.1976, C Loss: 204.1068\n",
            "[Epoch 43] D Loss: 909.0644, G Loss: 1427.0086, C Loss: 201.7238\n",
            "[Epoch 44] D Loss: 916.6809, G Loss: 1403.6448, C Loss: 196.9080\n",
            "[Epoch 45] D Loss: 916.1023, G Loss: 1399.4249, C Loss: 193.0062\n",
            "[Epoch 46] D Loss: 920.0039, G Loss: 1374.4771, C Loss: 194.1557\n",
            "[Epoch 47] D Loss: 921.2146, G Loss: 1373.3908, C Loss: 185.6040\n",
            "[Epoch 48] D Loss: 928.2395, G Loss: 1354.9857, C Loss: 185.8943\n",
            "[Epoch 49] D Loss: 930.7609, G Loss: 1351.5996, C Loss: 184.7511\n",
            "[Epoch 50] D Loss: 927.8686, G Loss: 1342.0355, C Loss: 181.1968\n",
            "[Epoch 51] D Loss: 936.5270, G Loss: 1326.7541, C Loss: 182.8891\n",
            "[Epoch 52] D Loss: 937.0504, G Loss: 1322.9068, C Loss: 179.5875\n",
            "[Epoch 53] D Loss: 931.2648, G Loss: 1325.9012, C Loss: 179.3490\n",
            "[Epoch 54] D Loss: 936.5494, G Loss: 1317.5424, C Loss: 180.2097\n",
            "[Epoch 55] D Loss: 936.8901, G Loss: 1310.2903, C Loss: 177.4130\n",
            "[Epoch 56] D Loss: 934.5092, G Loss: 1311.3027, C Loss: 180.3613\n",
            "[Epoch 57] D Loss: 942.7671, G Loss: 1296.9647, C Loss: 174.4940\n",
            "[Epoch 58] D Loss: 943.5354, G Loss: 1283.3655, C Loss: 173.6178\n",
            "[Epoch 59] D Loss: 940.5685, G Loss: 1281.3347, C Loss: 173.0920\n",
            "[Epoch 60] D Loss: 947.9893, G Loss: 1270.8629, C Loss: 165.5896\n",
            "[Epoch 61] D Loss: 944.2345, G Loss: 1273.8816, C Loss: 166.5121\n",
            "[Epoch 62] D Loss: 944.9335, G Loss: 1271.3309, C Loss: 166.3311\n",
            "[Epoch 63] D Loss: 945.9852, G Loss: 1267.3181, C Loss: 161.8138\n",
            "[Epoch 64] D Loss: 940.4654, G Loss: 1273.7661, C Loss: 162.7940\n",
            "[Epoch 65] D Loss: 942.3255, G Loss: 1270.5864, C Loss: 167.8227\n",
            "[Epoch 66] D Loss: 943.7555, G Loss: 1266.8970, C Loss: 158.6489\n",
            "[Epoch 67] D Loss: 942.9327, G Loss: 1261.9735, C Loss: 160.6537\n",
            "[Epoch 68] D Loss: 938.2235, G Loss: 1275.5091, C Loss: 164.4420\n",
            "[Epoch 69] D Loss: 936.9912, G Loss: 1271.6050, C Loss: 164.5768\n",
            "[Epoch 70] D Loss: 939.6709, G Loss: 1267.6456, C Loss: 159.1333\n",
            "[Epoch 71] D Loss: 940.7794, G Loss: 1279.9300, C Loss: 160.6145\n",
            "[Epoch 72] D Loss: 937.5826, G Loss: 1273.6711, C Loss: 166.1568\n",
            "[Epoch 73] D Loss: 937.1831, G Loss: 1274.0768, C Loss: 162.1619\n",
            "[Epoch 74] D Loss: 937.2889, G Loss: 1267.9037, C Loss: 157.7859\n",
            "[Epoch 75] D Loss: 933.2148, G Loss: 1274.0884, C Loss: 155.2155\n",
            "[Epoch 76] D Loss: 932.4985, G Loss: 1285.3994, C Loss: 162.3399\n",
            "[Epoch 77] D Loss: 934.4397, G Loss: 1279.9763, C Loss: 157.9185\n",
            "[Epoch 78] D Loss: 934.2745, G Loss: 1277.4257, C Loss: 154.6819\n",
            "[Epoch 79] D Loss: 934.6930, G Loss: 1275.1250, C Loss: 156.1757\n",
            "[Epoch 80] D Loss: 932.4314, G Loss: 1285.4184, C Loss: 154.6550\n",
            "[Epoch 81] D Loss: 930.4202, G Loss: 1278.8467, C Loss: 154.8791\n",
            "[Epoch 82] D Loss: 929.6359, G Loss: 1281.5967, C Loss: 153.9479\n",
            "[Epoch 83] D Loss: 927.1144, G Loss: 1285.0327, C Loss: 148.9911\n",
            "[Epoch 84] D Loss: 929.8559, G Loss: 1287.8318, C Loss: 151.9632\n",
            "[Epoch 85] D Loss: 925.4443, G Loss: 1290.4580, C Loss: 151.4865\n",
            "[Epoch 86] D Loss: 929.5215, G Loss: 1283.6131, C Loss: 150.4310\n",
            "[Epoch 87] D Loss: 926.7670, G Loss: 1281.1383, C Loss: 154.4524\n",
            "[Epoch 88] D Loss: 924.8468, G Loss: 1280.5973, C Loss: 152.6100\n",
            "[Epoch 89] D Loss: 925.5833, G Loss: 1291.5396, C Loss: 156.7827\n",
            "[Epoch 90] D Loss: 925.7781, G Loss: 1289.7787, C Loss: 152.5857\n",
            "[Epoch 91] D Loss: 931.0398, G Loss: 1282.0261, C Loss: 150.6899\n",
            "[Epoch 92] D Loss: 924.1781, G Loss: 1282.8782, C Loss: 153.3223\n",
            "[Epoch 93] D Loss: 922.6694, G Loss: 1283.8328, C Loss: 150.6029\n",
            "[Epoch 94] D Loss: 924.9611, G Loss: 1280.3295, C Loss: 149.4919\n",
            "[Epoch 95] D Loss: 929.3203, G Loss: 1278.7800, C Loss: 153.8881\n",
            "[Epoch 96] D Loss: 926.4182, G Loss: 1278.8324, C Loss: 151.3686\n",
            "[Epoch 97] D Loss: 923.6281, G Loss: 1287.3714, C Loss: 150.8380\n",
            "[Epoch 98] D Loss: 925.0693, G Loss: 1280.9194, C Loss: 151.8640\n",
            "[Epoch 99] D Loss: 927.1611, G Loss: 1276.0540, C Loss: 153.1127\n",
            "[Epoch 100] D Loss: 923.9592, G Loss: 1279.7912, C Loss: 151.7950\n",
            "\n",
            "‚úÖ Accuracy: 0.9392453951277481\n",
            "\n",
            "üìä Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        AMAZON       1.00      1.00      1.00      1584\n",
            "    CLOUDFLARE       0.98      0.96      0.97      1584\n",
            "       DROPBOX       0.93      0.93      0.93      1584\n",
            "      FACEBOOK       0.98      0.94      0.96      1584\n",
            "         GMAIL       0.93      0.98      0.95      1584\n",
            "        GOOGLE       0.94      0.92      0.93      1584\n",
            "          HTTP       1.00      1.00      1.00      1584\n",
            "  HTTP_CONNECT       0.96      0.88      0.92      1584\n",
            "    HTTP_PROXY       0.88      0.96      0.92      1584\n",
            "     MICROSOFT       0.96      0.98      0.97      1584\n",
            "           MSN       0.85      0.81      0.83      1584\n",
            "         SKYPE       0.85      0.92      0.89      1584\n",
            "           SSL       1.00      1.00      1.00      1584\n",
            "       TWITTER       0.93      0.93      0.93      1584\n",
            "WINDOWS_UPDATE       1.00      1.00      1.00      1584\n",
            "         YAHOO       0.82      0.86      0.84      1584\n",
            "       YOUTUBE       0.97      0.91      0.93      1584\n",
            "\n",
            "      accuracy                           0.94     26928\n",
            "     macro avg       0.94      0.94      0.94     26928\n",
            "  weighted avg       0.94      0.94      0.94     26928\n",
            "\n"
          ]
        }
      ]
    }
  ]
}